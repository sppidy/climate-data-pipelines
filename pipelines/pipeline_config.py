#!/usr/bin/env python3
"""
Configuration file for climate data processing pipelines
Generated by Terraform integration script
"""

import os

# AWS Configuration
AWS_REGION = "ap-south-1"  # Updated to match Terraform configuration
CLIMATE_DATA_BUCKET = "climate-data-dev-climate-data-b856a7c3"

# S3 Paths
S3_RAW_DATA_PATH = f"s3://{CLIMATE_DATA_BUCKET}/raw-data"
S3_PROCESSED_DATA_PATH = f"s3://{CLIMATE_DATA_BUCKET}/processed-data"
S3_TILES_PATH = f"s3://{CLIMATE_DATA_BUCKET}/tiles"

# Local paths (for EC2 instances)
LOCAL_DATA_DIR = "/opt/climate-data"
LOCAL_SCRIPTS_DIR = "/opt/climate-data/scripts"
LOCAL_OUTPUT_DIR = "/opt/climate-data/output"

# Processing configuration
DEFAULT_START_YEAR = 2022
DEFAULT_END_YEAR = 2025
DEFAULT_START_MONTH = 1
DEFAULT_END_MONTH = 12

# Data types to process
DATA_TYPES = ["precipitation", "humidity", "temperature"]

# Instance configuration
INSTANCE_TYPE = "t3.large"
MIN_INSTANCES = 0
MAX_INSTANCES = 5
DESIRED_INSTANCES = 1

# CloudFront configuration
CLOUDFRONT_ENABLED = True
CLOUDFRONT_PRICE_CLASS = "PriceClass_100"

# Logging configuration
LOG_LEVEL = "INFO"
LOG_FILE = "/var/log/climate-pipeline.log"

# Monitoring configuration
ENABLE_CLOUDWATCH_LOGS = True
ENABLE_SNS_NOTIFICATIONS = False

# Pipeline timeout settings (in seconds)
PIPELINE_TIMEOUT = 7200  # 2 hours
S3_SYNC_TIMEOUT = 1800   # 30 minutes

# Output directory patterns
OUTPUT_DIR_PATTERNS = {
    "precipitation": "precipitation_data_output",
    "humidity": "humidity_data_output", 
    "temperature": "temperature_data_output"
}

MBTILES_DIR_PATTERNS = {
    "precipitation": "precipitation_mbtiles_output",
    "humidity": "humidity_mbtiles_output",
    "temperature": "temperature_mbtiles_output"
}

def get_s3_path(data_type, file_type="processed"):
    """Get S3 path for specific data type and file type"""
    if file_type == "raw":
        return f"{S3_RAW_DATA_PATH}/{data_type}"
    elif file_type == "tiles":
        return f"{S3_TILES_PATH}/{data_type}"
    else:
        return f"{S3_PROCESSED_DATA_PATH}/{data_type}"

def get_local_path(data_type, file_type="processed"):
    """Get local path for specific data type and file type"""
    if file_type == "raw":
        return f"{LOCAL_DATA_DIR}/raw-data/{data_type}"
    elif file_type == "tiles":
        return f"{LOCAL_DATA_DIR}/tiles/{data_type}"
    else:
        return f"{LOCAL_DATA_DIR}/processed-data/{data_type}"

def get_output_dir(data_type):
    """Get output directory for specific data type"""
    return OUTPUT_DIR_PATTERNS.get(data_type, f"{data_type}_data_output")

def get_mbtiles_dir(data_type):
    """Get MBTiles directory for specific data type"""
    return MBTILES_DIR_PATTERNS.get(data_type, f"{data_type}_mbtiles_output")

def validate_data_type(data_type):
    """Validate if data type is supported"""
    return data_type in DATA_TYPES

def get_pipeline_timeout(data_type):
    """Get timeout for specific data type pipeline"""
    # Could be customized per data type if needed
    return PIPELINE_TIMEOUT
